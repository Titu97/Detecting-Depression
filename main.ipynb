{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc4f0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict, Dataset\n",
    "import torch.nn.functional as F\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    AutoConfig,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    TrainingArguments,\n",
    "    BitsAndBytesConfig,\n",
    "    Trainer)\n",
    "\n",
    " \n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0819dd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb40bd88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988abd71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb1060f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev=np.array([302, 307, 331, 335, 346, 367, 377, 381, 382, 388, 389, 390, 395,\n",
    "       403, 404, 406, 413, 417, 418, 420, 422, 436, 439, 440, 451, 458,\n",
    "       472, 476, 477, 482, 483, 484, 489, 490, 492])\n",
    "\n",
    "\n",
    "trainn=np.array([303, 304, 305, 310, 312, 313, 315, 316, 317, 318, 319, 320, 321,\n",
    "       322, 324, 325, 326, 327, 328, 330, 333, 336, 338, 339, 340, 341,\n",
    "       343, 344, 345, 347, 348, 350, 351, 352, 353, 355, 356, 357, 358,\n",
    "       360, 362, 363, 364, 366, 368, 369, 370, 371, 372, 374, 375, 376,\n",
    "       379, 380, 383, 385, 386, 391, 392, 393, 397, 400, 401, 402, 409,\n",
    "       412, 414, 415, 416, 419, 423, 425, 426, 427, 428, 429, 430, 433,\n",
    "       434, 437, 441, 443, 444, 445, 446, 447, 448, 449, 454, 455, 456,\n",
    "       457, 459, 463, 464, 468, 471, 473, 474, 475, 478, 479, 485, 486,\n",
    "       487, 488, 491])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd8756d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "\n",
    "model_checkpoint='facebook/opt-350m' \n",
    "tokenizerr = AutoTokenizer.from_pretrained(  model_checkpoint,add_prefix_space=True)\n",
    "\n",
    "\n",
    "def pad_sequences_pytorch(sequences, max_length=None, padding_value=0, padding='post'):\n",
    "    \"\"\"\n",
    "    Pad sequences to the same length with PyTorch, with optional truncating.\n",
    "\n",
    "    Args:\n",
    "        sequences (list of list of int): List of sequences, where each sequence is a list of integers.\n",
    "        max_length (int, optional): Maximum length of all sequences. If None, uses the length of the longest sequence. \n",
    "                                    Sequences longer than max_length will be truncated. Defaults to None.\n",
    "        padding_value (int, optional): Value used for padding. Defaults to 0.\n",
    "        padding (str, optional): Whether to pad 'pre' (before) or 'post' (after) the sequences. Defaults to 'post'.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: A tensor of shape (batch_size, max_sequence_length) containing the padded (and possibly truncated) sequences.\n",
    "    \"\"\"\n",
    "    if max_length is None:\n",
    "        max_length = max(len(seq) for seq in sequences)\n",
    "    \n",
    "    # Initialize an empty tensor for the padded sequences\n",
    "    padded_sequences = torch.full((len(sequences), max_length), padding_value, dtype=torch.int64)\n",
    "    \n",
    "    # Fill the tensor with the sequences\n",
    "    for i, seq in enumerate(sequences):\n",
    "        seq_length = min(len(seq), max_length)\n",
    "        seq_tensor = torch.tensor(seq[:seq_length], dtype=torch.int64)\n",
    "        if padding == 'pre':\n",
    "            # Pad at the beginning\n",
    "            padded_sequences[i, -seq_length:] = seq_tensor\n",
    "        else:\n",
    "            # Pad at the end (default)\n",
    "            padded_sequences[i, :seq_length] = seq_tensor\n",
    "            \n",
    "    return padded_sequences\n",
    "\n",
    "# Example usage\n",
    "sequences = [[1, 2, 3, 4, 5], [4, 5], [6, 7, 8, 9, 10, 11]]\n",
    "padded_sequences = pad_sequences_pytorch(sequences, max_length=2, padding_value=0, padding='post')\n",
    " \n",
    "file_path =  'depression7a.pkl'\n",
    " \n",
    "\n",
    "with open(file_path, 'rb') as file:\n",
    "    loaded_list = pickle.load(file)\n",
    "\n",
    "we=[]\n",
    "for i in range(len(loaded_list[0])):\n",
    "    \n",
    " \n",
    "    inputs = tokenizerr.encode(loaded_list[0][i], return_tensors=\"pt\",padding=True)\n",
    "    we.append(inputs[0].tolist())\n",
    "\n",
    "\n",
    "max_length  = 2048#max(len(seq) for seq in sequences)\n",
    "padded_sequences =  pad_sequences_pytorch(we, max_length=max_length, padding='post')\n",
    "inputs = torch.tensor(padded_sequences)\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "a,b,c,d,ee,f=train_test_split(torch.tensor(loaded_list[2]), inputs,torch.tensor(loaded_list[1]) ,test_size=0.2,stratify=torch.tensor(loaded_list[1]))\n",
    "\n",
    " \n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "\n",
    "ss = trainn \n",
    "indices = [i for i, value in enumerate(loaded_list[3]) if value in ss]\n",
    "\n",
    "\n",
    "a=torch.tensor(loaded_list[2])[indices]\n",
    "c=inputs[indices]\n",
    "ee=torch.tensor(loaded_list[1])[indices]\n",
    "\n",
    "\n",
    "\n",
    "ss = dev \n",
    "indices = [i for i, value in enumerate(loaded_list[3]) if value in ss]\n",
    "\n",
    "\n",
    "b=torch.tensor(loaded_list[2])[indices]\n",
    "d=inputs[indices]\n",
    "f=torch.tensor(loaded_list[1])[indices]\n",
    "\n",
    "\n",
    "\n",
    "dataset = TensorDataset(torch.tensor(a),torch.tensor(c),torch.tensor(ee))\n",
    "datasett = TensorDataset(b,d,f)\n",
    "loader = DataLoader(dataset, batch_size=5 , shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5bcdeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a338dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb02cfc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3ebb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e277d714",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "import base64\n",
    "import gzip\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, Iterable, Optional\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor, nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a6b3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Conv1d(nn.Conv1d):\n",
    "    \n",
    "    def _conv_forward(\n",
    "        self, x: Tensor, weight: Tensor, bias: Optional[Tensor]\n",
    "    ) -> Tensor:\n",
    "        return super()._conv_forward(\n",
    "            x, weight.to(x.dtype), None if bias is None else bias.to(x.dtype)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a90a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dropout_rate=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        \n",
    "        # Self attention for the query\n",
    "        self.self_attn = nn.MultiheadAttention(embed_dim=d_model, num_heads=nhead, dropout=dropout_rate)\n",
    "        \n",
    "        # Multi-head attention for query and key/value from encoder (x)\n",
    "        self.cross_attn = nn.MultiheadAttention(embed_dim=d_model, num_heads=nhead, dropout=dropout_rate)\n",
    "        \n",
    "        # Dropouts for post-attention\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        # Layer normalization\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        # Feed-forward network\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, 4 * d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(4 * d_model, d_model)\n",
    "        )\n",
    "        \n",
    "        # Another layer normalization for the feed-forward network\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, query, x, query_mask=None):\n",
    "        # Self attention\n",
    "        # print(query_mask.shape,\"skjdjkflskjlfmsd,mf,sdsssss\n",
    "        self_attn_output, _ =  self.self_attn(query, query, query, key_padding_mask=query_mask)\n",
    "        query = self.norm1(query + self.dropout1(self_attn_output))\n",
    "        \n",
    "        # Cross attention\n",
    "        # print(query.shape,x.shape)\n",
    "        attn_output, _ = self.cross_attn(query, x, x)\n",
    "        query = self.norm2(query + self.dropout2(attn_output))\n",
    "        \n",
    "        # Feed-forward network\n",
    "        ffn_output = self.ffn(query)\n",
    "        output = self.norm3(query + ffn_output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "class MultiLayerDecoder(nn.Module):\n",
    "    def __init__(self, d_model, nhead, query_dim, num_layers, max_seq_length, dropout_rate=0.0, num_classes=8):\n",
    "        super(MultiLayerDecoder, self).__init__()\n",
    "        \n",
    "        # Query transformation\n",
    "#         self.query_transform = nn.Linear(query_dim, d_model)\n",
    "        \n",
    "        # Decoder layers\n",
    "        self.layers = nn.ModuleList([DecoderLayer(d_model, nhead, dropout_rate) for _ in range(num_layers)])\n",
    "        \n",
    "        # Classification layer\n",
    "        # self.classifier = nn.Linear(d_model,  8)\n",
    "\n",
    "        # CLS token & position embedding\n",
    "        self.cls_token = nn.Parameter(torch.randn(1,1,d_model))\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, max_seq_length+1, d_model))\n",
    "        self.register_buffer(\"positional_embedding\", sinusoids(max_seq_length+1, d_model))\n",
    "    def forward(self, query, x ,query_lengths):\n",
    "        batch_size = query.size(0)\n",
    "        \n",
    "        # Transform query\n",
    "        \n",
    "#         query = self.query_transform(query)\n",
    "\n",
    "        # Add CLS token\n",
    "        cls_tokens = self.cls_token.repeat(batch_size, 1, 1)\n",
    "        query = torch.cat([cls_tokens, query], dim=1)\n",
    "        # print(query.size(1) ,\"kjlkjlk\")\n",
    "        # Add position embedding\n",
    "        query += self.positional_embedding#self.pos_embedding#[:, :query.size(1), :]\n",
    "        query = query.transpose(0, 1)  # Transposed for MHA compatibility\n",
    "        \n",
    "        # Adjust query_lengths for added CLS token\n",
    "        query_lengths += 1\n",
    "        \n",
    "        # Generate the mask based on query lengths\n",
    "        query_mask = create_mask(query_lengths, query.shape[0])\n",
    "        \n",
    "        # Pass query through each decoder layer\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            query = layer(query, x ,query_mask )\n",
    "        \n",
    "        # Using CLS token for classification\n",
    "        cls_output = query[0]\n",
    "        \n",
    "        \n",
    "        return cls_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f1a756",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sinusoids(length, channels, max_timescale=10000):\n",
    "    \"\"\"Returns sinusoids for positional embedding\"\"\"\n",
    "    assert channels % 2 == 0\n",
    "    log_timescale_increment = np.log(max_timescale) / (channels // 2 - 1)\n",
    "    inv_timescales = torch.exp(-log_timescale_increment * torch.arange(channels // 2))\n",
    "    scaled_time = torch.arange(length)[:, np.newaxis] * inv_timescales[np.newaxis, :]\n",
    "    return torch.cat([torch.sin(scaled_time), torch.cos(scaled_time)], dim=1)\n",
    " \n",
    "  \n",
    "def create_mask(seq_lengths, max_len):\n",
    "    mask = torch.arange(max_len).expand(len(seq_lengths), max_len).to(seq_lengths.device)\n",
    "    mask = mask >= seq_lengths.unsqueeze(-1)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c88a09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e41c89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class T(nn.Module):\n",
    "    def __init__(self  ):\n",
    "        super(T, self).__init__()\n",
    "       \n",
    "        model = whisper.load_model(\"base.en\")\n",
    "        #model=model.to(\"cuda:0\")\n",
    "        encoder=nn.DataParallel(model.encoder, device_ids = [0,1,2,3]) \n",
    "        encoder= encoder.to(\"cuda:0\")\n",
    "       \n",
    "        self.encoder=  encoder #model.encoder.to(\"cuda:0\")\n",
    "        d_model =  512\n",
    "        nhead = 16\n",
    "        query_dim = 512#3072#4096#2048\n",
    "        num_layers =   8\n",
    "        self.transform = nn.Linear(d_model, d_model)\n",
    "        self.decoder =   MultiLayerDecoder(d_model, nhead, query_dim, num_layers,2048)\n",
    "        \n",
    "        self.model =   AutoModelForCausalLM.from_pretrained( model_checkpoint ,pad_token_id=0,device_map=\"auto\",trust_remote_code=True)\n",
    "\n",
    " \n",
    "        self.conv1 =Conv1d(80,80, kernel_size=32,stride=30, padding=1)# Conv1d(n_mels,n_mels, kernel_size=7,stride=6, padding=1)\n",
    "       \n",
    "        \n",
    "\n",
    "\n",
    "    def forward(self, x,au):\n",
    "    \n",
    "     \n",
    "        x=self.encoder( self.conv1(x).to(\"cuda:0\") )\n",
    "        x=x.to(\"cpu\")\n",
    "        \n",
    "        \n",
    "        mask = (au != 0).int()\n",
    "        valid_lengths = [(row == 0).nonzero()[0][0].item() if (row == 0).any() else row.size(0) for row in mask]\n",
    "        \n",
    "       \n",
    "        \n",
    "        with torch.no_grad():\n",
    "            au =  self.model(au,mask )[0]\n",
    "        \n",
    "        \n",
    "        \n",
    " \n",
    "        x1= (x).transpose(0, 1)\n",
    "     \n",
    "        \n",
    "        logits = self.decoder(au,x1,torch.tensor(valid_lengths) )\n",
    "        out=self.transform(logits)\n",
    "      \n",
    "        return logits ,   out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3c6248",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b7709b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def extract_features(model, data_loader ):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    features = []\n",
    "    labels = []\n",
    "\n",
    "    with torch.no_grad():  # Turn off gradients to save memory and computations\n",
    "        for data  in data_loader:\n",
    "            target=data[2]\n",
    "            data = data#.to(device)\n",
    "            output,_ = model(data[0],data[1])  # Get the features from the model\n",
    "            features.extend(output.cpu().numpy())  # Store features\n",
    "            labels.extend(target.numpy())  # Store labels\n",
    "\n",
    "    return np.array(features), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6edd5df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03450222",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import random\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "ss = trainn#np.array( df1['Participant_ID'].tolist())\n",
    "indices = [i for i, value in enumerate(loaded_list[3]) if value in ss]\n",
    "\n",
    "\n",
    "X_train_=torch.tensor(loaded_list[2])[indices]\n",
    "X_train=inputs[indices]\n",
    "y_train=torch.tensor(loaded_list[1])[indices] \n",
    "\n",
    "\n",
    "ss = dev#np.array( df1['Participant_ID'].tolist())\n",
    "indices = [i for i, value in enumerate(loaded_list[3]) if value in ss]\n",
    "\n",
    "\n",
    "X_test_=torch.tensor(loaded_list[2])[indices]\n",
    "X_test=inputs[indices]\n",
    "y_test=torch.tensor(loaded_list[1])[indices] \n",
    "\n",
    "\n",
    "train_data = TensorDataset(X_train_,X_train, y_train)\n",
    "test_data = TensorDataset(X_test_,X_test, y_test)\n",
    " \n",
    "batch_size = 12  # Adjust as per your system's capability\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206063b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddccf5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "\n",
    "\n",
    "def compute_metrics(conf_matrix):\n",
    "    # Precision, Recall, and F1 for each class\n",
    "    TP = torch.diag(conf_matrix)\n",
    "    FP = conf_matrix.sum(0) - TP\n",
    "    FN = conf_matrix.sum(1) - TP\n",
    "    TN = conf_matrix.sum() - (TP + FP + FN)\n",
    "    \n",
    "    precision = TP / (TP + FP).clamp(min=1)  # Prevent division by zero\n",
    "    recall = TP / (TP + FN).clamp(min=1)\n",
    "    accuracy_per_class = (TP + TN) / conf_matrix.sum()  # Accuracy per class might not be typically reported\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall).clamp(min=1)\n",
    "    \n",
    "    # Overall accuracy\n",
    "    overall_accuracy = TP.sum() / conf_matrix.sum()\n",
    "    \n",
    "    # Handle divisions by zero\n",
    "    precision[precision != precision] = 0  # NaNs caused by zero division are set to 0\n",
    "    recall[recall != recall] = 0\n",
    "    f1_score[f1_score != f1_score] = 0\n",
    "    \n",
    "    return precision, recall, accuracy_per_class, f1_score, overall_accuracy\n",
    " \n",
    "\n",
    "\n",
    "def evall():\n",
    " \n",
    "    dropout=0.2\n",
    "    class NeuralNet(nn.Module):\n",
    "\n",
    "        def __init__(self):\n",
    "            super(NeuralNet, self).__init__()\n",
    "            self.fc1 = nn.Linear( 512, 1024)#512\n",
    "            self.batch_norm1 = nn.BatchNorm1d(1024)\n",
    "            self.dropout1 = nn.Dropout(dropout)\n",
    "            self.fc2 = nn.Linear(1024, 512)\n",
    "            self.batch_norm2 = nn.BatchNorm1d(512)\n",
    "            self.dropout2 = nn.Dropout(dropout)\n",
    "            self.fc3 = nn.Linear(span_length, 256)\n",
    "            self.batch_norm3 = nn.BatchNorm1d(256)\n",
    "            self.dropout3 = nn.Dropout(dropout)\n",
    "            self.fc4 = nn.Linear(256, 128)\n",
    "            self.batch_norm4 = nn.BatchNorm1d(128)\n",
    "            self.dropout4 = nn.Dropout(dropout)\n",
    "            self.fc5 = nn.Linear(128, 64)\n",
    "            self.batch_norm5 = nn.BatchNorm1d(64)\n",
    "            self.fc6 = nn.Linear(64, 2)\n",
    "\n",
    "        def forward(self, x):\n",
    "#             x = F.relu(self.batch_norm1(self.fc1(x)))\n",
    "#             x = self.dropout1(x)\n",
    "#             x = F.relu(self.batch_norm2(self.fc2(x)))\n",
    "#             x = self.dropout2(x)\n",
    "            x = F.relu(self.batch_norm3(self.fc3(x)))\n",
    "            x = self.dropout3(x)\n",
    "            x = F.relu(self.batch_norm4(self.fc4(x)))\n",
    "            x = self.dropout4(x)\n",
    "            x = F.relu(self.batch_norm5(self.fc5(x)))\n",
    "            x = self.fc6(x)\n",
    "            return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Initialize the model, loss function, and optimizer\n",
    "    \n",
    "    \n",
    "    ff=[]\n",
    "    wr=[]\n",
    "    for mm in range(300):\n",
    "        \n",
    "        \n",
    "        \n",
    "        total_length = 512\n",
    "        span_length =400#random.randint(350, 512)#300# 400\n",
    "\n",
    "        # Randomly choose a start position that allows a full span of 3000 to fit\n",
    "        start_position = random.randint(0, total_length - span_length)\n",
    "\n",
    "        # End position is start plus the span length\n",
    "        end_position = start_position + span_length\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        train_featuress,test_featuress=train_features[:,start_position:end_position],test_features[:,start_position:end_position]\n",
    "        \n",
    "        \n",
    "        #process_batch(torch.tensor(train_features)).numpy(),process_batch(torch.tensor(test_features)).numpy()\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        train_dataset = TensorDataset(torch.tensor(  train_featuress), torch.tensor(train_labels))\n",
    "\n",
    "        #test_features, test_labels = extract_features(model,  test_loader, device)\n",
    "        test_dataset = TensorDataset(torch.tensor(test_featuress), torch.tensor(test_labels))\n",
    "\n",
    "        train_loaderr = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "        test_loaderr = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "        modell =NeuralNet() \n",
    "        modell=modell.to(\"cuda:1\")\n",
    "\n",
    "\n",
    "\n",
    "        modell.train()\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.Adam(modell.parameters(), lr=0.00001,weight_decay=0.2)\n",
    "\n",
    "        # Training loop\n",
    "        num_epochs  =200\n",
    "        for epoch in range(num_epochs):\n",
    "            for inputs, labels in train_loaderr:\n",
    "                # Forward pass\n",
    "                inputs, labels=inputs.to(\"cuda:1\"),labels.to(\"cuda:1\")\n",
    "                outputs = modell(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                # Backward and optimize\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "\n",
    "                optimizer.step()\n",
    "\n",
    "        #     print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "        # Evaluate the model\n",
    "        modell.eval()\n",
    "        with torch.no_grad():\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for inputs, labels in test_loaderr:\n",
    "                inputs, labels=inputs.to(\"cuda:1\"),labels.to(\"cuda:1\")\n",
    "                outputs = modell(inputs)\n",
    "                probabilities = F.softmax(outputs, dim=-1)\n",
    "                \n",
    "                wr.append(probabilities)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        accuracy = correct / total\n",
    "        #print(accuracy)\n",
    "\n",
    "\n",
    "\n",
    "        f1 = f1_score(labels.cpu(), predicted.cpu(), average='macro')\n",
    "\n",
    "        #print(f'F1 score of kNN classifier on test set: {f1:.2f}')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        ff.append( predicted.cpu())\n",
    "\n",
    "    \n",
    "    stacked_predictions = torch.stack(ff, dim=0)\n",
    "        \n",
    "       \n",
    "\n",
    "    majority_vote = torch.sum(stacked_predictions, dim=0) > (stacked_predictions.size(0) / 2)\n",
    "\n",
    "\n",
    "    wee=[]\n",
    "    \n",
    "    majority_vote = majority_vote.int()\n",
    "    wee.append(majority_vote)\n",
    "    \n",
    "    \n",
    " \n",
    "    f1 = f1_score(  labels.cpu(),     majority_vote, average='macro')\n",
    "    print(f1,\"majority vote\")\n",
    "    \n",
    "    \n",
    "    num_classes=2\n",
    "    conf_matrix = torch.zeros(num_classes, num_classes, dtype=torch.int64)\n",
    "    for t, p in zip( labels.cpu(),   majority_vote):\n",
    "        conf_matrix[t, p] += 1\n",
    "\n",
    "    # Compute metrics\n",
    "    precision, recall, accuracy_per_class, f1, overall_acc = compute_metrics(conf_matrix)\n",
    "    print(\"Precision:\", precision, \"Recall:\", recall, \"Accuracy per class:\", accuracy_per_class, \"F1:\", f1, \"Overall Accuracy:\", overall_acc)\n",
    "  \n",
    "    ss=torch.stack(wr, dim=0)\n",
    "    aa=torch.max(ss, 0)[0] \n",
    "    \n",
    "    pr=torch.max(aa, 1)[1]\n",
    "    \n",
    "    wee.append(pr.cpu())\n",
    "    f1 = f1_score(  labels.cpu(),     pr.cpu(), average='macro')\n",
    "    print(f1,\"max prob.\")\n",
    "    \n",
    "    \n",
    "    \n",
    "     \n",
    "    aa=0\n",
    "    for i in range(len(wr)):\n",
    "        aa=aa+wr[i]\n",
    "    \n",
    "    pr=torch.max(aa, 1)[1]\n",
    "    wee.append(pr.cpu())\n",
    "    f1 = f1_score(  labels.cpu(),     pr.cpu(), average='macro')\n",
    "    print(f1,\"summ prob.\")\n",
    " \n",
    "    stacked_predictions = torch.stack(wee, dim=0)\n",
    "    majority_vote = torch.sum(stacked_predictions, dim=0) > (stacked_predictions.size(0) / 2)\n",
    "    majority_vote = majority_vote.int()\n",
    "  \n",
    "    \n",
    "\n",
    "    f1 = f1_score(  labels.cpu(),     majority_vote, average='macro')\n",
    "    print(f1,\"avg. of all ensemble\")\n",
    "     \n",
    "    return wr\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87fb4656",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467d7d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as n\n",
    "\n",
    "\n",
    "def torch_kron(a, b):\n",
    "    a_shape = [a.size(0), a.size(1)]\n",
    "    b_shape = [b.size(0), b.size(1)]\n",
    "    return torch.reshape(torch.reshape(a, [a_shape[0], 1, a_shape[1], 1]) * torch.reshape(b, [1, b_shape[0], 1, b_shape[1]]), [a_shape[0] * b_shape[0], a_shape[1] * b_shape[1]])\n",
    "\n",
    "\n",
    "\n",
    "class Identity(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Identity, self).__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "\n",
    "h= feature=512#1024#512#1024#50272#128256#51200#1024#768#1024#768#5376#1024#8064000#768#1024\n",
    "n=2\n",
    "labell = torch.arange(n, dtype=torch.int64)#.to(f'cuda:{model.device_ids[0]}') #.to(f'cuda:{x.device_ids[0]}')\n",
    "p = torch.tensor(np.ones((n, 1)), dtype=torch.float32)#.to(f'cuda:{model.device_ids[0]}') #.to(f'cuda:{x.device_ids[0]}')\n",
    "ones_tensor = torch.tensor(np.ones((1, n)), dtype=torch.float32)#.to(f'cuda:{model.device_ids[0]}') #.to(f'cuda:{x.device_ids[0]}')\n",
    "A=np.identity(n) \n",
    "L=np.kron(A,np.ones([h,1])) \n",
    "M = torch.tensor(L, dtype=torch.float32)#.to(f'cuda:{model.device_ids[0]}') #.to(f'cuda:{x.device_ids[0]}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451e6043",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f2cf48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0133d299",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model=T()\n",
    "model.model.lm_head= Identity()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6071844",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    " \n",
    "\n",
    "optimizer = torch.optim.AdamW([\n",
    "    {'params': model.conv1.parameters()},\n",
    "    {'params': model.encoder.parameters()},\n",
    "    {'params': model.decoder.parameters()}\n",
    "], lr=0.00001)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model.train()\n",
    "for i in range(1201):\n",
    "\n",
    "    \n",
    "  \n",
    "    optimizer.zero_grad() \n",
    "    loss=0\n",
    "    for mm in range(2):\n",
    "        ww=[]\n",
    "        wwe=[]\n",
    "        selected_indices = np.random.choice(2,2 , replace=False)\n",
    "        for ii in range(2):\n",
    "            # Find indices where the value is equal to 3\n",
    "            indices_where_equal_to_3 = torch.nonzero( torch.tensor(y_train) == selected_indices[ii], as_tuple=False)\n",
    "\n",
    "\n",
    "            # Your tensor\n",
    "            your_tensor =  indices_where_equal_to_3\n",
    "\n",
    "            # Generate random indices\n",
    "            random_indices = np.random.choice( your_tensor.size(0), 2, replace=False)  #torch.randperm(your_tensor.size(0))[:2]\n",
    "\n",
    "            # Select two random numbers\n",
    "            selected_numbers = your_tensor[random_indices]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            ww.append(selected_numbers[0])\n",
    "            wwe.append(selected_numbers[1])\n",
    "\n",
    "        dd=ww+wwe\n",
    "        dt = np.stack([tensor.numpy() for tensor in  dd])\n",
    "        dt=dt.T\n",
    "\n",
    "\n",
    "\n",
    "        inputss=X_train[dt][0]#.to(f'cuda:{model.device_ids[0]}') \n",
    "        _,tensor2 = model(X_train_[dt][0],inputss )\n",
    "\n",
    "        t1=tensor2[0:n]\n",
    "        t2=tensor2[n:n*2]\n",
    "\n",
    "\n",
    "        z = torch.matmul(p, t1.reshape(1, n*h)) - torch_kron(ones_tensor, t2)\n",
    "\n",
    "\n",
    "        logits = -torch.matmul(torch.square(z), M)\n",
    "\n",
    "        loss =  loss+  loss_fn(logits,labell)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(loss,i)\n",
    "    if i%100==0 and i>0   :\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "         \n",
    "\n",
    "        train_features, train_labels = extract_features(model,  train_loader ) \n",
    "        test_features, test_labels = extract_features(model,  test_loader )\n",
    "        \n",
    "        evall()\n",
    "        model.train()\n",
    "        \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8045aae6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254a104f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9aaee8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2052db36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ba26ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac49892b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "x",
   "language": "python",
   "name": "x"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
